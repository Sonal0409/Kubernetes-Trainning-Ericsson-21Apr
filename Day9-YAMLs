ETCD Backup:
================================

Install etcdctl:
  # export RELEASE="3.3.13"
  #  wget https://github.com/etcd-io/etcd/releases/download/v${RELEASE}/etcd-v${RELEASE}-linux-amd64.tar.gz
 #   tar xvf etcd-v${RELEASE}-linux-amd64.tar.gz
 #   cd etcd-v${RELEASE}-linux-amd64
#   sudo mv etcdctl /usr/local/bin

etcd snapshot explanation
the idea is to create a snapshot of the etcd database. This is done by communicating with the running etcd instance in Kubernetes and asking it to create a snapshot. in order to communicate with the etcd pod in Kubernetes, we need to:
	Use the host network in order to access 127.0.0.1:2379, where etcd is exposed (--network host)
	Specify the correct etcd API version as environment variable (--env ETCDCTL_API=3)
	The actual command for creating a snapshot (etcdctl snapshot save /backup/etcd-snapshot-latest.db)
		Some flags for the etcdctl command
		Specify where to connect to (--endpoints=https://127.0.0.1:2379)
		Specify certificates to use (--cacert=..., --cert=..., --key=...)

backup ETCD Data:


mkdir etcd-backup

# pwd

# kubectl get pods -o wide -n kube-system


Copy the ipaddress of etcd pod -> thats the endpoint


In the below query replace the ipaddress with the ipaddress of etcd pod
10.128.0.6


# sudo ETCDCTL_API=3 etcdctl --endpoints=<Etcdpod_IP>:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot save /home/labuser/etcd-backup/etcd-snapshot-latest.db


remove /var/lib/etcd folder, delete couple of deployments/pods

rm -rf /var/lib/etcd
restore ETCD Data:

In the below query replace the ipaddress with the ipaddress of etcd pod




# sudo ETCDCTL_API=3 etcdctl snapshot restore /home/labuser/etcd-backup/etcd-snapshot-latest.db --initial-cluster etcd-restore=https://<IP_ETCD_POD>:2380 --initial-advertise-peer-urls=https://<IP_ETCD_POD>:2380 --name etcd-restore --data-dir /var/lib/etcd


If you get an error that etcd directory already exists

# rm -rf /var/lib/etcd

Run the restore command again


# kubectl get all


=========================================================================
Kubernetes dashboard:

Creating a token and deploy kubernetes  dashboard 



Deploy the dashboard:

# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# kubectl get all  -n kubernetes-dashboard

# kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard

Chnage the type of service in spec from ClusterIP to NodePort

Save the file

Verifying the changes

kubectl get svc -n kubernetes-dashboard -o wide

Accessing Kubernetes Dashboard

Open browser 
Public IP of any master/worker node:<<NodePort>>
Example: https://35.239.229.82:30189/

Click on Advanced -> Accept Risk and Continue


To access the dashboard we need a token for which we have to create a Service account

vim serviceaccount.yml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: sandry
  namespace: kubernetes-dashboard


# kubectl create -f serviceaccount.yml


# vim clusterrole-sa.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
 - kind: ServiceAccount
   name: sandry
   namespace: kubernetes-dashboard



# kubectl create -f clusterrole-sa.yml



# vim sercret-sa.yml

apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
 name: mysecret-sa
 namespace: kubernetes-dashboard
 annotations:
  kubernetes.io/service-account.name: sandry


# kubectl create -f sercret-sa.yml

# kubectl describe secret   mysecret-sa -n kubernetes-dashboard

Copy the token and then paste on the dashboard page to loginto it.

You will be able to see the kubernetes dashboard now.

